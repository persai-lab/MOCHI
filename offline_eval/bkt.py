# Written by Chunpai W., rewritten based on Shayan's code.

# Contains a class to model a BKT-based model
# (currently loads one outputted by Michael Yudelson's code,

#   In the following functions and class, the BKT model is primarily defined using the
#   following data structures.

#   O - a 3-d numpy matrix.
#       The first index specifies the action (or KCs),
#       the second specifies the state (0 for unmastered, 1 for mastered),
#       the third specifies the observation (0 for incorrect, 1 for correct)
#   T - a 3-d numpy matrix.
#       The first index specifies the action (or KCs),
#       the second specifies the current state (0 for unmastered, 1 for mastered),
#       the third specifies the next state (0 for unmastered, 1 for mastered).
#   pi - a 2-d numpy matrix.
#       The first index specifies the action (or KCs),
#       the second specifies the state (0 for unmastered, 1 for mastered).

import numpy as np
import copy
import itertools


def getYudelsonModel(filename, numKCs, kcs):
    """
    Parses the output of Michael Yudelson's code and outputs a BKT model.
      Takes in a model output file generated by Michael Yudelson's BKT code, and outputs
      O, T, and pi as specified above.
      It also outputs kcMap, which is a dictionary mapping KC names to integers as generated
      by Michael Yudelson's code.

    :param filename:
    :param numKCs:
    :param kcs:
    :return:
    """
    kc_list = copy.deepcopy(kcs)
    with open(filename, 'r') as file1:
        pi = np.zeros((numKCs, 2))
        T = np.zeros((numKCs, 2, 2))
        O = np.zeros((numKCs, 2, 2))
        kcMap = {}
        for _ in range(7):  # skip first 7 lines
            file1.readline()
        for i in range(numKCs):
            kc = file1.readline().strip().split()
            if kc == []: break
            while ';' in kc[1]:
                for _ in range(3):
                    file1.readline()
                kc = file1.readline().strip().split()
            kcMap[kc[1]] = i  # map kc name to kc id
            kc_list.remove(kc[1])
            probs = file1.readline().strip().split()
            pi[i, :] = [float(probs[2]), float(probs[1])]  # priors, reverse the order
            probs = file1.readline().strip().split()
            T[i, :, :] = [[float(probs[4]), float(probs[3])], [float(probs[2]), float(probs[1])]]
            probs = file1.readline().strip().split()
            O[i, :, :] = [[float(probs[4]), float(probs[3])], [float(probs[2]), float(probs[1])]]
        if i < numKCs - 1:  # if the file does not output all KC parameters
            for j in range(numKCs - i):
                kcMap[kc_list[0]] = i + j
                kc_list.remove(kc_list[0])
                pi[i + j, :] = [0.8, 0.2]
                T[i + j, :, :] = [[0.9, 0.1], [0, 1]]
                O[i + j, :, :] = [[0.9, 0.1], [0.1, 0.9]]
    return (O, T, pi, kcMap)


def getWCRPModel(model_filename, skill_map_filename, steps2kcs, numKCs):
    """
    Note: In the WCRP model, KC names are integers, but these integers won't necessarily
    be the same as the ones outputted by Michael Yudelson's code.
    :param model_filename:
    :param skill_map_filename:
    :param steps2kcs:
    :param numKCs:
    :return:
    """
    with open(skill_map_filename, 'r') as skill_map_file:
        skill_ids = [int(x) for x in skill_map_file.readline().strip().split()]

    # make a new steps2kcs mapping to map steps to their WCRP learned skill labels.
    new_steps2kcs = {}
    for step in steps2kcs.keys():
        new_steps2kcs[step] = skill_ids[steps2kcs[step]]

    # extract the model parameters
    pi = np.zeros((numKCs, 2))
    T = np.zeros((numKCs, 2, 2))
    O = np.zeros((numKCs, 2, 2))
    kcMap = {}
    with open(model_filename, 'r') as model_file:
        for i in range(numKCs):
            kc = int(model_file.readline().strip())
            kcMap[kc] = i
            PL0 = float(model_file.readline().strip().split()[1])
            PL = float(model_file.readline().strip().split()[1])
            PF = float(model_file.readline().strip().split()[1])
            PG = float(model_file.readline().strip().split()[1])
            PS = 1 - float(model_file.readline().strip().split()[1])
            if 1 - PS < PG:
                if (PL > PF):
                    print("no learning")
                PL, PF = PF, PL
                PS, PG = 1 - PG, 1 - PS
            else:
                if (PL < PF):
                    print("no learning")
            pi[i, :] = [1 - PL0, PL0]
            T[i, :, :] = [[1 - PL, PL], [PF, 1 - PF]]
            O[i, :, :] = [[1 - PG, PG], [PS, 1 - PS]]
    return (O, T, pi, kcMap, new_steps2kcs)


def compute_bkt_last_belief(pi, O, T, records, kcMap, problem_topic_mapping):
    """
    new belief is updated once we see a new observation
    :param pi:
    :param O:
    :param T:
    :param records: a user's historical records
    :param kcMap:
    :return:
    """
    numactions = np.shape(T)[0]
    latest_belief = copy.copy(pi)
    for (q, a) in records:
        kc = kcMap[problem_topic_mapping[q]]
        tmp_bel = np.zeros((numactions, 2))
        for j in [0, 1]:
            alphasum = 0
            for i in [0, 1]:
                alphasum += latest_belief[kc, i] * T[kc, i, j]
            # not known belief with incorrect answer = (unknown * to-unknown + known * 0) * (1-p(G))
            # known belief with incorrect answer = (unknown * known + known * 1) * p(S)

            # not known belief with correct answer = (unknown * to-unknown + known * 0) * p(G)
            # known belief with correct answer = (unknown * known + known * 1) * (1-p(S))
            tmp_bel[kc, j] = alphasum * O[kc, j, a]
        latest_belief[kc, :] = tmp_bel[kc, :] / np.sum(tmp_bel[kc, :])  # normalized the belief
    return latest_belief[:, 1]


class BKTModel():
    """
    A class used to model a BKT simulator as well as to plan according to the BKT mastery policy.
    (Potentially more policies/variants of the mastery policy will be added later on.)
    """

    def __init__(self, O, T, pi, kcMap, rewardModel, problemList, problem_topic_mapping, top_k=1,
                 mastery_thresh=0.95):
        self.type = self.__class__.__name__
        self.O = O
        self.T = T
        self.pi = pi
        self.kcMap = kcMap
        self.rewardModel = rewardModel
        self.policies = {'mastery': self.mastery_policy,
                         'random': self.random_policy,
                         'highest_prob_correct': self.highest_prob_correct_policy,
                         'myopic': self.myopic_policy}
        self.sampleInitBKT()  # initialize the BKT states self.curr_state randomly
        self.curr_bel = copy.deepcopy(self.pi)
        self.initProblemList = copy.deepcopy(problemList)
        self.problemList = copy.deepcopy(problemList)
        self.problem_topic_mapping = copy.deepcopy(problem_topic_mapping)
        self.top_k = top_k
        self.pretests = []
        self.actions = []
        self.rewards = []
        self.finalRewards = []
        self.finalBeliefs = []
        self.mastery_thresh = mastery_thresh

    def resetState(self, pretest=None, time_percentile=None):
        """
        reset the state and prior beliefs
        :param pretest:
        :param time_percentile:
        :return:
        """
        self.sampleInitBKT()
        self.curr_bel = copy.deepcopy(self.pi)  # every test user should have same init beliefs
        self.problemList = copy.deepcopy(self.initProblemList)
        self.pretests.append(pretest)
        self.time_percentile = time_percentile
        self.actions.append([])
        self.rewards.append([])

    def updateState(self, step_outcomes):
        self.updateBelief(step_outcomes, update=True)

    def updateBelief(self, step_outcomes, update=True):
        numactions = np.shape(self.T)[0]
        new_bel = copy.deepcopy(self.curr_bel)
        for t in range(len(step_outcomes)):
            a = self.kcMap[self.problem_topic_mapping[step_outcomes[t][0]]]
            o = step_outcomes[t][1]
            tmp_bel = np.zeros((numactions, 2))
            for j in [0, 1]:
                alphasum = 0
                for i in [0, 1]:
                    alphasum += new_bel[a, i] * self.T[a, i, j]
                tmp_bel[a, j] = alphasum * self.O[a, j, o]
            new_bel[a, :] = tmp_bel[a, :] / np.sum(tmp_bel[a, :])
        if update:
            self.curr_bel = copy.deepcopy(new_bel)

        return new_bel

    # Made to interface with Tutorshop calls to external_selector
    def setBelief(self, kc_list):
        for kc_params in kc_list:
            kc = self.kcMap[kc_params["name"]]
            self.curr_bel[kc, :] = kc_params["p_known"]

    def sampleInitBKT(self):
        numKCs = np.shape(self.pi)[0]
        init_state = np.zeros(numKCs)
        for kc in range(numKCs):
            init_state[kc] = np.random.choice(2, p=self.pi[kc, :])
        self.curr_state = copy.deepcopy(init_state)

    def sampleState(self, action):
        """
        use trained transition matrix to sample next state
        use trained observation matrix to sample next observation or correctness
        :param action: a problem
        :return:
        """
        next_state = copy.deepcopy(self.curr_state)
        o = []
        topic = self.problem_topic_mapping[action]
        kc = self.kcMap[topic]
        next_state[kc] = np.random.choice(2, p=self.T[kc, int(self.curr_state[kc]), :])
        # time spent is 0
        # random choice based on updated probability of current state
        o.append((action, np.random.choice(2, p=self.O[kc, int(self.curr_state[kc]), :]), 0))
        self.curr_state = copy.deepcopy(next_state)
        self.updateBelief(o, update=True)
        self.actions[-1].append(action)
        self.rewards[-1].append(0)
        return o

    def plan(self, policy):
        action = self.policies[policy](self.problemList)
        self.actions[-1].append(action)
        self.rewards[-1].append(0)
        # self.problemList.remove(action)
        return action

    def updateReward(self):
        if self.rewardModel != None:
            if None in self.pretests:
                # update reward for plan model
                test_X = self.curr_bel[:, 1].reshape(1, -1)
                if self.rewardModel.type == "LogisticModel":
                    correct_prob = self.rewardModel.predict(test_X)[-1][-1]
                elif self.rewardModel.type == "LinearModel":
                    correct_prob = self.rewardModel.predict(test_X)[-1]
                    if correct_prob > 1:
                        correct_prob = 1
                    elif correct_prob < 0:
                        correct_prob = 0
                else:
                    raise AttributeError
                self.finalRewards.append(correct_prob)
            else:
                # use pretest score and current belief to predict the post score for student model
                test_X = np.append([self.pretests[-1]], self.curr_bel[:, 1]).reshape(1, -1)
                if self.rewardModel.type == "LogisticModel":
                    correct_prob = self.rewardModel.predict(test_X)[-1][-1]
                elif self.rewardModel.type == "LinearModel":
                    correct_prob = self.rewardModel.predict(test_X)[-1]
                    if correct_prob > 1:
                        correct_prob = 1
                    elif correct_prob < 0:
                        correct_prob = 0
                else:
                    raise AttributeError
                self.finalRewards.append(correct_prob)
        else:
            self.finalRewards.append(0)
        self.finalBeliefs.append(self.curr_bel[:, 1])

    def mastery_policy(self, problemList):
        scores = {}
        for problem in problemList:
            scores[problem] = 0.0
            topic = self.problem_topic_mapping[problem]
            kc = self.kcMap[topic]
            # print(problem, self.curr_bel[kc, 1])
            if self.curr_bel[kc, 1] < self.mastery_thresh:
                scores[problem] += self.mastery_thresh - self.curr_bel[kc, 1]
        sorted_scores = sorted(scores.keys(), key=lambda x: scores[x])
        return sorted_scores[-self.top_k:][::-1]

    def random_policy(self, problemList):
        # return np.random.choice(problemList)
        return np.random.choice(problemList, self.top_k, replace=False)

    def highest_prob_correct_policy(self, problemList):
        scores = {}
        for problem in problemList:
            scores[problem] = 0.0
            topic = self.problem_topic_mapping[problem]
            kc = self.kcMap[topic]

            # compute the prob of correctness
            # (curr mastery belief * 1 + unmastery belief * prob. mastery) * non-slip correct obs +
            # (curr unmastery belief * unmastery transition + curr. mastery * 0) * guess correct obs
            scores[problem] = (
                    (self.curr_bel[kc, 1] * self.T[kc, 1, 1] + self.curr_bel[kc, 0] * self.T[
                        kc, 0, 1]) * self.O[kc, 1, 1] + (
                            self.curr_bel[kc, 0] * self.T[kc, 0, 0] + self.curr_bel[kc, 1] *
                            self.T[kc, 1, 0]) * self.O[kc, 0, 1]
            )
        sorted_scores = sorted(scores.keys(), key=lambda x: scores[x])
        return sorted_scores[-self.top_k:][::-1]

    def myopic_policy(self, problemList, num_samples=20):
        scores = {}
        for problem in problemList:
            scores[problem] = 0.0
            for sample in range(num_samples):
                topic = self.problem_topic_mapping[problem]
                kc = self.kcMap[topic]
                o = [(problem, np.random.choice(2, p=self.O[kc, int(self.curr_state[kc]), :]), 0)]
                tmp_bel = self.updateBelief(o, update=False)
                tmp_X = np.append([self.pretests[-1]], tmp_bel[:, 1]).reshape(1, -1)
                scores[problem] += self.rewardModel.predict(tmp_X)[-1]
            scores[problem] /= num_samples
        sorted_scores = sorted(scores.keys(), key=lambda x: scores[x])
        return sorted_scores[-self.top_k:][::-1]
